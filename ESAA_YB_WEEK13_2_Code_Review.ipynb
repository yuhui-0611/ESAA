{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuhui-0611/ESAA/blob/main/ESAA_YB_WEEK13_2_Code_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **식음업장 메뉴 수요 예측 AI 오프라인 해커톤**"
      ],
      "metadata": {
        "id": "vNAcUPDl5MiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://dacon.io/competitions/official/236594/codeshare/12883?page=1&dtype=recent)"
      ],
      "metadata": {
        "id": "PyNLMmVV5S9c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKUikxNGlZvS"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y887r2ilZvU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_EPGDMplZvV"
      },
      "source": [
        "# Fixed RandomSeed & Setting Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhyhlV1ilZvV"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "LOOKBACK, PREDICT, BATCH_SIZE, EPOCHS = 28, 7, 64, 50\n",
        "LR = 1e-3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"DEVICE:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SkJ_P8MlZvV"
      },
      "source": [
        "# Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFfKet_llZvW"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"./\"\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
        "TEST_DIR = os.path.join(BASE_DIR, \"test\")\n",
        "\n",
        "train = pd.read_csv(os.path.join(TRAIN_DIR, \"train.csv\"), parse_dates=[\"영업일자\"])\n",
        "\n",
        "# 메타 경로\n",
        "TRAIN_META_DIR = os.path.join(TRAIN_DIR, \"meta\")\n",
        "TEST_META_DIR  = os.path.join(TEST_DIR, \"meta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFHMOI3glZvW"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFbotr7-lZvW"
      },
      "outputs": [],
      "source": [
        "# [메타 데이터 IO/병합]\n",
        "META_OPTIONS = {\n",
        "    \"ski\":     {\"use\": True,  \"cols\": [\"1일내장객\"]},\n",
        "    \"room\":    {\"use\": True,  \"cols\": None},\n",
        "    \"hwadam\":  {\"use\": True,  \"cols\": None},\n",
        "    \"weather\": {\"use\": True,  \"cols\": None},\n",
        "    \"price\":   {\"use\": True,  \"cols\": [\"평균판매금액\"]},\n",
        "}\n",
        "\n",
        "def _read_if_exists(path, parse_dates=None):\n",
        "    import pandas as pd, os\n",
        "    if os.path.exists(path):\n",
        "        return pd.read_csv(path, parse_dates=parse_dates)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def _normalize_weather_cols(df):\n",
        "    \"\"\"TRAIN_weather / TEST_weather 파일의 '일시'를 '영업일자'로 바꾸고\n",
        "       날짜만 남겨 일(day) 단위로 조인되게 정규화.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    if '일시' in df.columns:\n",
        "        df = df.copy()\n",
        "        df['영업일자'] = pd.to_datetime(df['일시']).dt.date\n",
        "        df.drop(columns=['일시'], inplace=True)\n",
        "        # 다시 Timestamp 컬럼으로 맞춤 (train/test의 '영업일자'가 Timestamp라면)\n",
        "        df['영업일자'] = pd.to_datetime(df['영업일자'])\n",
        "    return df\n",
        "\n",
        "def read_train_meta(train_meta_dir, train_dir):\n",
        "    ski = _read_if_exists(os.path.join(train_meta_dir, \"TRAIN_ski.csv\"),     parse_dates=[\"영업일자\"])\n",
        "    room= _read_if_exists(os.path.join(train_meta_dir, \"TRAIN_room.csv\"),    parse_dates=[\"영업일자\"])\n",
        "    hw  = _read_if_exists(os.path.join(train_meta_dir, \"TRAIN_hwadam.csv\"),  parse_dates=[\"영업일자\"])\n",
        "    w   = _read_if_exists(os.path.join(train_meta_dir, \"TRAIN_weather.csv\"), parse_dates=[\"일시\"])\n",
        "    w   = _normalize_weather_cols(w)\n",
        "\n",
        "    price = _read_if_exists(os.path.join(train_dir, \"price.csv\"))\n",
        "    return ski, room, hw, w, price\n",
        "\n",
        "\n",
        "def read_test_meta(test_meta_dir, test_prefix):\n",
        "    suffix = test_prefix.split('_')[-1]\n",
        "    ski = _read_if_exists(os.path.join(test_meta_dir, f\"TEST_ski_{suffix}.csv\"),     parse_dates=[\"영업일자\"])\n",
        "    room= _read_if_exists(os.path.join(test_meta_dir, f\"TEST_room_{suffix}.csv\"),    parse_dates=[\"영업일자\"])\n",
        "    hw  = _read_if_exists(os.path.join(test_meta_dir, f\"TEST_hwadam_{suffix}.csv\"),  parse_dates=[\"영업일자\"])\n",
        "    w   = _read_if_exists(os.path.join(test_meta_dir, f\"TEST_weather_{suffix}.csv\"), parse_dates=[\"일시\"])\n",
        "    w   = _normalize_weather_cols(w)\n",
        "    return ski, room, hw, w\n",
        "\n",
        "\n",
        "def merge_meta(df, ski, room, hwadam, weather, price, options=None):\n",
        "    if options is None:\n",
        "        options = META_OPTIONS\n",
        "    out = df.copy()\n",
        "\n",
        "    def _merge_numeric_by_date(base, meta_df, allowed_cols=None):\n",
        "        if meta_df is None or meta_df.empty:\n",
        "            return base\n",
        "        meta_df = meta_df.copy()\n",
        "\n",
        "        # 1) 후보 컬럼 선택: allowed_cols가 주어지면 그 교집합만, 아니면 모든 숫자 컬럼\n",
        "        if allowed_cols:\n",
        "            num_cols = [c for c in allowed_cols if c in meta_df.columns]\n",
        "        else:\n",
        "            num_cols = [c for c in meta_df.columns\n",
        "                        if c != \"영업일자\" and np.issubdtype(meta_df[c].dtype, np.number)]\n",
        "\n",
        "        if not num_cols:\n",
        "            return base\n",
        "\n",
        "        # 2) 숫자 변환(문자 숫자 방지), 결측 처리\n",
        "        meta_df[num_cols] = meta_df[num_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
        "\n",
        "        return base.merge(meta_df[[\"영업일자\"] + num_cols], on=\"영업일자\", how=\"left\")\n",
        "\n",
        "    # 날짜 기반 메타 병합\n",
        "    if options.get(\"ski\", {}).get(\"use\", False):\n",
        "        out = _merge_numeric_by_date(out, ski,     options.get(\"ski\", {}).get(\"cols\"))\n",
        "    if options.get(\"room\", {}).get(\"use\", False):\n",
        "        out = _merge_numeric_by_date(out, room,    options.get(\"room\", {}).get(\"cols\"))\n",
        "    if options.get(\"hwadam\", {}).get(\"use\", False):\n",
        "        out = _merge_numeric_by_date(out, hwadam,  options.get(\"hwadam\", {}).get(\"cols\"))\n",
        "    if options.get(\"weather\", {}).get(\"use\", False):\n",
        "        out = _merge_numeric_by_date(out, weather, options.get(\"weather\", {}).get(\"cols\"))\n",
        "\n",
        "    # price\n",
        "    if options.get(\"price\", {}).get(\"use\", False) and price is not None and not price.empty:\n",
        "        keep_cols = options[\"price\"].get(\"cols\") or [c for c in price.columns if c != \"영업장명_메뉴명\"]\n",
        "        cols = [\"영업장명_메뉴명\"] + [c for c in keep_cols if c in price.columns and c != \"영업장명_메뉴명\"]\n",
        "        out = out.merge(price[cols], on=\"영업장명_메뉴명\", how=\"left\")\n",
        "\n",
        "    out = out.sort_values([\"영업장명_메뉴명\", \"영업일자\"]).reset_index(drop=True)\n",
        "    out.fillna(0, inplace=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "def select_meta_columns(df) :\n",
        "    exclude = {\"영업일자\", \"영업장명_메뉴명\", \"매출수량\"}\n",
        "    numeric_cols = [c for c in df.columns if c not in exclude and np.issubdtype(df[c].dtype, np.number)]\n",
        "    return numeric_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ed-dNOlZvX"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyd0ZwmelZvX"
      },
      "outputs": [],
      "source": [
        "class MultiOutputLSTMWithMeta(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, output_dim=7, meta_dim=0):\n",
        "        super().__init__()\n",
        "        self.use_meta = meta_dim > 0\n",
        "        self.lstm = nn.LSTM(input_dim + meta_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, meta=None):\n",
        "        if self.use_meta and meta is not None:\n",
        "            x = torch.cat([x, meta], dim=-1)   # (B,L,1+M)\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])         # (B,H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GvInuZTlZvX"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uqAUB4nlZvX"
      },
      "outputs": [],
      "source": [
        "def train_lstm_meta(train_df, train_meta_dir):\n",
        "    trained_models = {}\n",
        "    ski, room, hw, w, price = read_train_meta(train_meta_dir, TRAIN_DIR)\n",
        "\n",
        "    merged = merge_meta(train_df, ski, room, hw, w, price, options=META_OPTIONS)\n",
        "    meta_cols_all = select_meta_columns(merged)\n",
        "\n",
        "    for store_menu, group in tqdm(merged.groupby(['영업장명_메뉴명']), desc='Training LSTM+Meta'):\n",
        "        key = store_menu[0] if isinstance(store_menu, tuple) else store_menu\n",
        "        store_train = group.sort_values('영업일자').copy()\n",
        "        if len(store_train) < LOOKBACK + PREDICT:\n",
        "            continue\n",
        "\n",
        "        sales_scaler = MinMaxScaler()\n",
        "        store_train['매출수량'] = sales_scaler.fit_transform(\n",
        "            store_train[['매출수량']].values\n",
        "        )  # <- np.ndarray\n",
        "\n",
        "        meta_scaler = None\n",
        "        if meta_cols_all:\n",
        "            meta_scaler = MinMaxScaler()\n",
        "            store_train[meta_cols_all] = meta_scaler.fit_transform(\n",
        "                store_train[meta_cols_all].values\n",
        "            )  # <- np.ndarray\n",
        "\n",
        "        # 시퀀스 구성\n",
        "        X_train, M_train, y_train = [], [], []\n",
        "        sales_vals = store_train[['매출수량']].values  # (N,1) np.ndarray\n",
        "        meta_vals  = store_train[meta_cols_all].values if meta_cols_all else None\n",
        "\n",
        "        for i in range(len(sales_vals) - LOOKBACK - PREDICT + 1):\n",
        "            X_train.append(sales_vals[i:i+LOOKBACK])\n",
        "            if meta_vals is not None:\n",
        "                M_train.append(meta_vals[i:i+LOOKBACK])\n",
        "            y_train.append(sales_vals[i+LOOKBACK:i+LOOKBACK+PREDICT, 0])\n",
        "\n",
        "        X_train = torch.tensor(np.array(X_train)).float().to(DEVICE)\n",
        "        y_train = torch.tensor(np.array(y_train)).float().to(DEVICE)\n",
        "        M_train = torch.tensor(np.array(M_train)).float().to(DEVICE) if meta_vals is not None else None\n",
        "\n",
        "        meta_dim = (M_train.size(-1) if M_train is not None else 0)\n",
        "        model = MultiOutputLSTMWithMeta(input_dim=1, output_dim=PREDICT, meta_dim=meta_dim).to(DEVICE)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(EPOCHS):\n",
        "            idx = torch.randperm(len(X_train), device=DEVICE)\n",
        "            for i in range(0, len(X_train), BATCH_SIZE):\n",
        "                batch_idx = idx[i:i+BATCH_SIZE]\n",
        "                X_batch = X_train[batch_idx]\n",
        "                y_batch = y_train[batch_idx]\n",
        "                M_batch = M_train[batch_idx] if M_train is not None else None\n",
        "\n",
        "                output = model(X_batch, M_batch)\n",
        "                loss = criterion(output, y_batch)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        trained_models[key] = {\n",
        "            'model': model.eval(),\n",
        "            'sales_scaler': sales_scaler,\n",
        "            'meta_scaler': meta_scaler,\n",
        "            'meta_cols': meta_cols_all\n",
        "        }\n",
        "\n",
        "    return trained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HnKkHI7lZvX"
      },
      "outputs": [],
      "source": [
        "# 학습\n",
        "trained_models = train_lstm_meta(train, TRAIN_META_DIR)\n",
        "print(\"학습 완료. 모델 수:\", len(trained_models))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AousC8MlZvY"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfcaRGsHlZvY"
      },
      "outputs": [],
      "source": [
        "def make_meta_test(test_df, test_meta_dir, test_prefix, meta_cols_train):\n",
        "    ski_t, room_t, hw_t, w_t = read_test_meta(test_meta_dir, test_prefix)\n",
        "\n",
        "    merged = test_df.copy()\n",
        "    merged[\"영업일자\"] = pd.to_datetime(merged[\"영업일자\"])\n",
        "\n",
        "    def _merge_numeric_by_date(base, meta_df, allowed_cols=None):\n",
        "        if meta_df is None or meta_df.empty:\n",
        "            return base\n",
        "        meta_df = meta_df.copy()\n",
        "        meta_df[\"영업일자\"] = pd.to_datetime(meta_df[\"영업일자\"])\n",
        "        if allowed_cols:\n",
        "            num_cols = [c for c in allowed_cols if c in meta_df.columns]\n",
        "        else:\n",
        "            num_cols = [c for c in meta_df.columns if c != \"영업일자\" and np.issubdtype(meta_df[c].dtype, np.number)]\n",
        "        if not num_cols:\n",
        "            return base\n",
        "        meta_df[num_cols] = meta_df[num_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
        "        return base.merge(meta_df[[\"영업일자\"] + num_cols], on=\"영업일자\", how=\"left\")\n",
        "\n",
        "    if META_OPTIONS[\"ski\"][\"use\"]:\n",
        "        merged = _merge_numeric_by_date(merged, ski_t, META_OPTIONS[\"ski\"][\"cols\"])\n",
        "    if META_OPTIONS[\"room\"][\"use\"]:\n",
        "        merged = _merge_numeric_by_date(merged, room_t, META_OPTIONS[\"room\"][\"cols\"])\n",
        "    if META_OPTIONS[\"hwadam\"][\"use\"]:\n",
        "        merged = _merge_numeric_by_date(merged, hw_t, META_OPTIONS[\"hwadam\"][\"cols\"])\n",
        "    if META_OPTIONS[\"weather\"][\"use\"]:\n",
        "        merged = _merge_numeric_by_date(merged, w_t, META_OPTIONS[\"weather\"][\"cols\"])\n",
        "\n",
        "    # price 병합\n",
        "    price = None\n",
        "    if META_OPTIONS.get(\"price\", {}).get(\"use\", False):\n",
        "        _, _, _, _, price = read_train_meta(TRAIN_META_DIR, TRAIN_DIR)\n",
        "    if price is not None and not price.empty and \"영업장명_메뉴명\" in price.columns:\n",
        "        keep = [\"영업장명_메뉴명\"] + [c for c in (META_OPTIONS[\"price\"][\"cols\"] or []) if c in price.columns]\n",
        "        if len(keep) == 1:  # cols=None일 때는 숫자 전부\n",
        "            keep = [\"영업장명_메뉴명\"] + [c for c in price.columns if c != \"영업장명_메뉴명\" and np.issubdtype(price[c].dtype, np.number)]\n",
        "        merged = merged.merge(price[keep], on=\"영업장명_메뉴명\", how=\"left\")\n",
        "\n",
        "    # 학습 때 사용한 meta 컬럼 목록을 가져옴\n",
        "    meta_cols_used = list(meta_cols_train) if meta_cols_train else []\n",
        "    if meta_cols_used:\n",
        "        for c in meta_cols_used:\n",
        "            if c not in merged.columns:\n",
        "                merged[c] = 0.0\n",
        "        merged[meta_cols_used] = merged[meta_cols_used].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    item_to_seq = {}\n",
        "    for item, g in merged.groupby(\"영업장명_메뉴명\"):\n",
        "        g = g.sort_values(\"영업일자\").tail(LOOKBACK)\n",
        "        X28 = g[\"매출수량\"].values.astype(float).reshape(1, LOOKBACK, 1)\n",
        "        M28 = g[meta_cols_used].values.astype(float).reshape(1, LOOKBACK, -1) if meta_cols_used else None\n",
        "        item_to_seq[item] = (X28, M28)\n",
        "\n",
        "    return item_to_seq, meta_cols_used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUoz-YkClZvY"
      },
      "outputs": [],
      "source": [
        "def predict_lstm_meta(test_df, trained_models, test_prefix):\n",
        "    results = []\n",
        "    global TEST_META_DIR\n",
        "\n",
        "    if not trained_models:\n",
        "        return pd.DataFrame(columns=[\"영업일자\",\"영업장명_메뉴명\",\"매출수량\"])\n",
        "\n",
        "    any_key = next(iter(trained_models))\n",
        "    meta_cols_train = trained_models[any_key][\"meta_cols\"]\n",
        "\n",
        "    item_to_seq, meta_cols_used = make_meta_test(\n",
        "        test_df=test_df,\n",
        "        test_meta_dir=TEST_META_DIR,\n",
        "        test_prefix=test_prefix,\n",
        "        meta_cols_train=meta_cols_train,\n",
        "    )\n",
        "\n",
        "    for item in tqdm(test_df[\"영업장명_메뉴명\"].unique(), desc=f\"Predicting {test_prefix}\", leave=True):\n",
        "        if item not in trained_models or item not in item_to_seq:\n",
        "            continue\n",
        "\n",
        "        x28, m28 = item_to_seq[item]\n",
        "        bundle   = trained_models[item]\n",
        "        model    = bundle[\"model\"]\n",
        "        s_scaler = bundle[\"sales_scaler\"]\n",
        "        m_scaler = bundle[\"meta_scaler\"]\n",
        "\n",
        "        # 스케일 변환\n",
        "        x28_s = s_scaler.transform(x28.reshape(-1, 1)).reshape(x28.shape)\n",
        "        m28_s = None\n",
        "        if (m28 is not None) and (m_scaler is not None):\n",
        "            m28_s = m_scaler.transform(m28.reshape(-1, m28.shape[-1])).reshape(m28.shape)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_t = torch.tensor(x28_s, dtype=torch.float32, device=DEVICE)\n",
        "            m_t = torch.tensor(m28_s, dtype=torch.float32, device=DEVICE) if m28_s is not None else None\n",
        "            pred_s = model(x_t, m_t).squeeze().cpu().numpy()  # (PREDICT,)\n",
        "\n",
        "        # 역변환\n",
        "        for i, d in enumerate([f\"{test_prefix}+{k+1}일\" for k in range(PREDICT)]):\n",
        "            val = s_scaler.inverse_transform([[pred_s[i]]])[0, 0]\n",
        "            results.append({\n",
        "                \"영업일자\": d,\n",
        "                \"영업장명_메뉴명\": item,\n",
        "                \"매출수량\": max(val, 0.0)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5o17KLBlZvY"
      },
      "outputs": [],
      "source": [
        "# 모든 TEST_*.csv 순회\n",
        "all_preds = []\n",
        "\n",
        "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\")))\n",
        "\n",
        "for path in test_files:\n",
        "    test_df = pd.read_csv(path, parse_dates=[\"영업일자\"])\n",
        "    filename = os.path.basename(path)\n",
        "    test_prefix = re.search(r'(TEST_\\d+)', filename).group(1)\n",
        "    pred_df = predict_lstm_meta(test_df, trained_models, test_prefix)\n",
        "    all_preds.append(pred_df)\n",
        "\n",
        "full_pred_df = pd.concat(all_preds, ignore_index=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "seohee",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}